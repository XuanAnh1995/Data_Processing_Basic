{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfc2b66-ce35-4b13-baf2-962e48800fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"\n",
    "<p>This is an example text with some <b>HTML tags</b> and special characters like &amp;.</p>\n",
    "It also has some extra spaces  and mixed case.\n",
    "Let's see how we can clean it!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255dcd44-8923-46fe-9994-8f9fe58819e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0246768-57fe-4e22-a18a-fc2650d17ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: this is an example text with some html tags and special characters like amp it also has some extra spaces and mixed case lets see how we can clean it\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Loại bỏ HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Loại bỏ ký tự đặc biệt và dấu câu\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "    # Xóa khoảng trắng thừa\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(\"Cleaned text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a4a7e0-0a51-4ee0-9c94-3459e34f0b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized text: this is an example text with some html tags and special characters like amp it also has some extra spaces and mixed case lets see how we can clean it\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Chuẩn hóa Unicode\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Ở đây có thể thêm xử lý lỗi chính tả nếu cần\n",
    "    return text\n",
    "\n",
    "normalized_text = normalize_text(cleaned_text)\n",
    "print(\"Normalized text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb6afe3-2251-4111-83cd-2f96a2060713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removed text: example text html tags special characters like amp also extra spaces mixed case lets see clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luuvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "stopword_removed_text = remove_stopwords(normalized_text)\n",
    "print(\"Stopword removed text:\", stopword_removed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24371b2e-3ad9-4f46-abf8-c121f5fe4415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luuvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text: example text html tag special character like amp also extra space mixed case let see clean\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "lemmatized_text = lemmatize_text(stopword_removed_text)\n",
    "print(\"Lemmatized text:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f18e79e-5d72-48a6-9073-5d11d71a49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\luuvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74872c3-c8c9-459d-8971-ba8a33debb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagged text: [('example', 'NOUN'), ('text', 'NOUN'), ('html', 'PROPN'), ('tag', 'NOUN'), ('special', 'ADJ'), ('character', 'NOUN'), ('like', 'ADP'), ('amp', 'PROPN'), ('also', 'ADV'), ('extra', 'ADJ'), ('space', 'NOUN'), ('mixed', 'ADJ'), ('case', 'NOUN'), ('let', 'AUX'), ('see', 'VERB'), ('clean', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "#Giả sử lemmatized_text đã được định nghĩa.\n",
    "spacy_pos_tagged_text = spacy_pos_tagging(lemmatized_text)\n",
    "print(\"spaCy POS tagged text:\", spacy_pos_tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d643d36-f930-4b3b-8a82-730720497039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER entities: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_ner(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "ner_entities = extract_ner(normalized_text) #Dùng normalized text thì hơn\n",
    "print(\"NER entities:\", ner_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cad54b-497a-4c6e-abc7-df23a4c5ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'label': 'NEGATIVE', 'score': 0.9427444338798523}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return result\n",
    "\n",
    "sentiment = analyze_sentiment(normalized_text) #Dùng normalized text thì hơn\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b61188-7f25-4762-8e69-1ba7cb617a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['this', 'is', 'an', 'example', 'text', 'with', 'some', 'html', 'tags', 'and', 'special', 'characters', 'like', 'amp', 'it', 'also', 'has', 'some', 'extra', 'spaces', 'and', 'mixed', 'case', 'lets', 'see', 'how', 'we', 'can', 'clean', 'it']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "def bert_tokenize(text):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "tokenized_text = bert_tokenize(normalized_text) #Dùng normalized text thì hơn\n",
    "print(\"Tokenized text:\", tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e65350c-2e4e-402f-a8bf-407dbcc627cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2023, 2003, 2019, 2742, 3793, 2007, 2070, 16129, 22073, 1998, 2569, 3494, 2066, 23713, 2009, 2036, 2038, 2070, 4469, 7258, 1998, 3816, 2553, 11082, 2156, 2129, 2057, 2064, 4550, 2009]\n"
     ]
    }
   ],
   "source": [
    "def convert_tokens_to_ids(tokens):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return input_ids\n",
    "\n",
    "token_ids = convert_tokens_to_ids(tokenized_text)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e043c74-5fe4-401f-ba1b-219d72bc0742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
