{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3af11f-c1fb-4562-97ef-65eec85feb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07030469179153442, 0.0006977436714805663, 0.03368493914604187, -0.07487841695547104, -0.026996184140443802]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Sử dụng mô hình BERT (SBERT) để tạo embedding\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Tạo embedding từ văn bản\n",
    "text = \"LangChain là một framework mạnh mẽ cho AI.\"\n",
    "vector = embedding_model.embed_query(text)\n",
    "\n",
    "print(vector[:5])  # Hiển thị 5 giá trị đầu của vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7241f26e-e81c-4548-94ea-77d89cd54701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f6a024-b78f-4d4a-aac8-2ec6a2b8b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af922cba-4e7f-40e5-9db0-1ddc7de53337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044d8ad3-53ad-4a5d-b71a-7251f4109ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-bert not in index list\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo Pinecone\n",
    "pc = Pinecone(api_key= os.environ.get(\"PINECONE_API_KEY\"), environment = os.environ.get(\"PINECONE_ENV\"))\n",
    "\n",
    "# Tạo index nếu chưa có\n",
    "index_name = \"langchain-bert\"\n",
    "dimension = 384\n",
    "metric = \"cosine\"\n",
    "\n",
    "if index_name in [index.name for index in pc.list_indexes()]:\n",
    "    pc.delete_index(index_name)\n",
    "    print(f\"{index_name} successfully delete\")\n",
    "else :\n",
    "    print(f\"{index_name} not in index list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "830bc75b-ecab-474f-b463-85c127592ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(\"my-index-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa22e596-4c67-4db5-901c-e9aa0b2a920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"langchain-bert\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"langchain-bert-03idc8k.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.create_index(\n",
    "    name= index_name,\n",
    "    dimension= dimension,\n",
    "    metric= metric,\n",
    "    spec= ServerlessSpec(\n",
    "        cloud= \"aws\",\n",
    "        region= \"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "885ab4f3-e831-4464-b4e7-cb3598447289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết nối đến index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c5624a-a7be-44e9-9091-46e453e7bb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thêm dữ liệu vào Pinecone\n",
    "vector_id = \"doc_1\"\n",
    "index.upsert(vectors=[(vector_id, vector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e51ddbd-3326-4ab1-bafc-8b6e42f14214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top kết quả: {'matches': [{'id': 'doc_1', 'score': 0.537551463, 'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "query_vector = embedding_model.embed_query(\"Công cụ AI mạnh mẽ\")\n",
    "\n",
    "# Thực hiện truy vấn trong Pinecone\n",
    "results = index.query(vector=query_vector, top_k=3, include_metadata=True)\n",
    "\n",
    "print(\"Top kết quả:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2ea0194-4626-4242-99cd-15dd5e465e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46aeedc1-19a8-4531-a12c-1f786a62ec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458a3a290eff4362b084e54b74b39b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a410b09ac4489e8938af1878c2c1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd4219d1e334d16af0522f3e4dfb0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cad55cd82d4efda47d515118d60f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86921495f4b423ca821b99d1946913e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae7eede55244c439a6a205c5410fc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Tải mô hình T5 để tóm tắt văn bản\n",
    "t5_pipeline = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e8680d-1f07-4270-854e-177a27ccb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tích hợp vào LangChain\n",
    "t5_model = HuggingFacePipeline(pipeline=t5_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7d260b-d06b-4d2b-9db4-007eab75909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử nghiệm với văn bản dài\n",
    "long_text = \"\"\"\n",
    "LangChain là một framework hỗ trợ xây dựng ứng dụng AI dựa trên mô hình ngôn ngữ lớn (LLM).\n",
    "Nó giúp kết nối LLM với cơ sở dữ liệu, API, và nhiều công cụ khác để xây dựng chatbot, hệ thống hỏi đáp, và tìm kiếm thông minh.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cb8f349-caa3-4aa8-9c36-a9ed06febb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 174. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tóm tắt: LangChain là mt framework h tr xây dng ng AI . nó gip kt ni LLM và nhiu công c khác .\n"
     ]
    }
   ],
   "source": [
    "summary = t5_model(long_text)\n",
    "print(\"Tóm tắt:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63e75a39-5025-4898-b380-87508a90f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "237408f7-e347-41a8-b2cf-18d41ddede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tích hợp vào LangChain\n",
    "qa_model = HuggingFacePipeline(pipeline=qa_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b49c20ab-5a6c-49fa-b822-3555c70b42a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu trả lời: LangChain dùng  làm g?\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChain dùng để làm gì?\"\n",
    "answer = qa_model(question)\n",
    "print(\"Câu trả lời:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47a04e25-763b-4480-bb25-3a20c8e715e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo vector database từ Pinecone\n",
    "vectorstore = Pinecone(embedding_model, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fa94e8-adfa-4fd3-88ed-38f2264cc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c09fb-bad2-4b53-bf7d-576e7d1257a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
