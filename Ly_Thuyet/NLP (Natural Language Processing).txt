üìå NLP (Natural Language Processing)
‚îú‚îÄ‚îÄ üî§ Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (Text Preprocessing)
‚îÇ ‚îú‚îÄ‚îÄ üìè L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning): regex, stopwords, stemming, lemmatization
‚îÇ ‚îú‚îÄ‚îÄ üî° M√¥ h√¨nh th·ªëng k√™ (Statistical Models): TF-IDF, N-gram
‚îÇ ‚îú‚îÄ‚îÄ üè∑Ô∏è G√°n nh√£n t·ª´ lo·∫°i (POS Tagging)
‚îÇ ‚îú‚îÄ‚îÄ üîç Nh·∫≠n d·∫°ng th·ª±c th·ªÉ c√≥ t√™n (NER - Named Entity Recognition)
‚îÇ ‚îú‚îÄ‚îÄ üìñ Ph√¢n t√≠ch ng·ªØ nghƒ©a (Semantic Analysis), t·ªïng h·ª£p vƒÉn b·∫£n (Text Generation)
‚îÇ
‚îú‚îÄ‚îÄ üí° M√¥ h√¨nh NLP truy·ªÅn th·ªëng (Traditional NLP Models)
‚îÇ ‚îú‚îÄ‚îÄ üìñ TF-IDF (Term Frequency - Inverse Document Frequency): X√°c ƒë·ªãnh t·∫ßm quan tr·ªçng c·ªßa t·ª´
‚îÇ ‚îú‚îÄ‚îÄ üî† Word Embeddings (Bi·ªÉu di·ªÖn t·ª´ d∆∞·ªõi d·∫°ng vector)
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üîó Word2Vec: CBOW, Skip-gram
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üöÄ FastText: H·ªó tr·ª£ t·ª´ kh√¥ng ph·ªï bi·∫øn (OOV)
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üìå GloVe (Global Vectors for Word Representation)
‚îÇ
‚îú‚îÄ‚îÄ üöÄ Deep Learning trong NLP
‚îÇ ‚îú‚îÄ‚îÄ üß† Transformers - Cu·ªôc c√°ch m·∫°ng NLP
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üî• Attention Mechanism - C·ªët l√µi c·ªßa Transformers
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üöÄ Self-Attention - Hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n RNN
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üåç Multi-Head Attention - TƒÉng kh·∫£ nƒÉng h·ªçc t·ª´ d·ªØ li·ªáu
‚îÇ ‚îú‚îÄ‚îÄ üîó Deep Neural Networks (DNNs) - M·∫°ng n∆°-ron s√¢u: 		Input layer, Hidden layer, Output layer
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üèóÔ∏è Feedforward Neural Networks (FNNs): 				1 h∆∞·ªõng input -> hidden -> output
‚îÇ   ‚îú‚îÄ‚îÄ üìö Multi-Layer Perceptron (MLP): 				(Input Layer) ‚Üí (Hidden Layer 1) ‚Üí (Hidden Layer 2) ‚Üí ... ‚Üí (Output Layer)
‚îÇ ‚îú‚îÄ‚îÄ üì∑ Convolutional Neural Network (CNN) - Nh·∫≠n di·ªán ·∫£nh:	
‚îÇ ‚îú‚îÄ‚îÄ üîÑ Recurrent Neural Network (RNN) - M·∫°ng h·ªìi quy
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ‚è≥ Long Short-Term Memory (LSTM):				Forget gate, Input gate, Output gate
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üîÑ Gated Recurrent Unit (GRU):					Reset gate, Update gate 
‚îÇ ‚îú‚îÄ‚îÄ üîÑ Autoencoders (AE) - M·∫°ng t·ª± m√£ h√≥a
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üèóÔ∏è Encoder - M√£ h√≥a d·ªØ li·ªáu : Chuy·ªÉn ƒë·∫ßu v√†o x -> latent representation
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üõ†Ô∏è Decoder - Gi·∫£i m√£ d·ªØ li·ªáu: D√πng latent representation t√°i t·∫°o d·ªØ li·ªáu x' -> H√†m m·∫•t m√°t (Loss Function)
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üìé Variational Autoencoder (VAE) - Sinh d·ªØ li·ªáu m·ªõi
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üßº Denoising Autoencoder (DAE) - X·ª≠ l√Ω nhi·ªÖu
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ üõ† Fine-tuning m√¥ h√¨nh Pretrained tr√™n t·∫≠p d·ªØ li·ªáu ri√™ng
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üîß Hu·∫•n luy·ªán m√¥ h√¨nh cho nhi·ªám v·ª• ƒë·∫∑c th√π
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ üî• T√πy ch·ªânh theo ·ª©ng d·ª•ng: Chatbot, Ph√¢n lo·∫°i vƒÉn b·∫£n,...
‚îÇ
‚îú‚îÄ‚îÄ üîé ·ª®ng d·ª•ng th·ª±c t·∫ø c·ªßa NLP
‚îÇ ‚îú‚îÄ‚îÄ üó£Ô∏è D·ªãch ng√¥n ng·ªØ (Google Translate, DeepL)
‚îÇ ‚îú‚îÄ‚îÄ ü§ñ Chatbots (ChatGPT, tr·ª£ l√Ω ·∫£o Google Assistant, Siri)
‚îÇ ‚îú‚îÄ‚îÄ üìä Ph√¢n t√≠ch c·∫£m x√∫c (Social Media Monitoring, Customer Feedback)
‚îÇ ‚îú‚îÄ‚îÄ üì∞ T√≥m t·∫Øt vƒÉn b·∫£n t·ª± ƒë·ªông (Summarization)
‚îÇ ‚îú‚îÄ‚îÄ üîç H·ªá th·ªëng t√¨m ki·∫øm th√¥ng minh (Google Search, Elasticsearch)
‚îÇ ‚îú‚îÄ‚îÄ üéôÔ∏è Nh·∫≠n di·ªán gi·ªçng n√≥i (Speech-to-Text, Voice Assistants)


1:	NLP (Natural Languge Processing): x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n: L√† 1 nh√°nh c·ªßa ng√¥n ng·ªØ h·ªçc, khoa h·ªçc m√°y t√≠nh v√† tr√≠ tu·ªá nh√¢n t·∫°o ; 
	li√™n quan ƒë·∫øn s·ª± t∆∞∆°ng t√°c gi·ªØa m√°y t√≠nh v√† ng√¥n ng·ªØ t·ª± nhi√™n c·ªßa con ng∆∞·ªùi, gi·ªçng n√≥i, ho·∫∑c vƒÉn b·∫£n.

1.1. 	Text Preprocessing: Ti·ªÅn x·ª≠ l√Ω gi√∫p chu·∫©n b·ªã d·ªØ li·ªáu, vƒÉn b·∫£n ƒë·ªÉ m√¥ h√¨nh NLP ho·∫°t ƒë·ªông t·ªët h∆°n
	1.1.1: 	L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning):
			Regex (Regular Expressions): d√πng bi·ªÉu th·ª©c ch√≠nh quy ƒë·ªÉ chu·∫©n h√≥a vƒÉn b·∫£n (lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, d·∫•u c√¢u, ...)
			Stopwords: B·ªè nh·ªØng t·ª´ kh√¥ng quan tr·ªçng 
			Stemming: C·∫Øt g·ªëc t·ª´ ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng t·ª´ v·ª±ng
			Lemmatization: gi·ªØ l·∫°i g·ªëc t·ª´ nh∆∞ng d∆∞·ªõi d·∫°ng c√≥ nghƒ©a h∆°n so v·ªõi stemming

	1.1.2: 	M√¥ h√¨nh th·ªëng k√™ (Statistical Models):
			TF-IDF (Term Frequency - Inverse Document Frequency): X√°c ƒë·ªãnh m·ª©c ƒë·ªô quan tr·ªçng c·ªßa t·ª´ trong m·ªôt vƒÉn b·∫£n so v·ªõi to√†n b·ªô t·∫≠p d·ªØ li·ªáu
			N-gram: Ph√¢n ƒëo·∫°n vƒÉn b·∫£n th√†nh nh√≥m t·ª´ li√™n ti·∫øp (unigram, bigram, trigram...)

	1.1.3: 	G√°n nh√£n t·ª´ lo·∫°i (POS Tag)
			X√°c ƒë·ªãnh danh t·ª´, ƒë·ªông t·ª´, t√≠nh t·ª´, ... gi√∫p hi·ªÉu ng·ªØ ph√°p c·ªßa c√¢u

	1.1.4: 	Nh·∫≠n d·∫°ng th·ª±c th·ªÉ c√≥ t√™n (NER _ Named Entity Recognition)
			X√°c ƒë·ªãnh c√°c th·ª±c th·ªÉ quan tr·ªçng nh∆∞ t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c, ng√†y th√°ng.

	1.1.5: 	Ph√¢n t√≠ch ng·ªØ nghƒ©a (Semantic) v√† T·ªïng h·ª£p vƒÉn b·∫£n (Text Generation)
			Semantic Analysis: Hi·ªÉu √Ω nghƒ©a c·ªßa t·ª´ v√† c√¢u (v√≠ d·ª•: ph√¢n t√≠ch c·∫£m x√∫c)
			Text Generation: Sinh ra vƒÉn b·∫£n m·ªõi t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o (GPT, T5)


1.2: 	M√¥ h√¨nh NLP truy·ªÅn th·ªëng (Traditional NLP Models)

	1.2.1: 	TF-IDF :ƒê∆∞·ª£c d√πng ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ vƒÉn b·∫£n d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´.
		Word Embeddings (Bi·ªÉu di·ªÖn t·ª´ d·∫°ng vector): Thay v√¨ xem t·ª´ nh∆∞ m·ªôt chu·ªói k√Ω t·ª±, ta bi·ªÉu di·ªÖn n√≥ b·∫±ng vector s·ªë ƒë·ªÉ m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c ng·ªØ nghƒ©a.
			Word2Vec (CBOW & Skip-gram):
				CBOW: D·ª± ƒëo√°n t·ª´ d·ª±a v√†o ng·ªØ c·∫£nh xung quanh
				Skip-gram: D·ª± ƒëo√°n ng·ªØ c·∫£nh d·ª±a v√†o t·ª´ trung t√¢m
			FastText : C·∫£i ti·∫øn t·ª´ Word2Vec, h·ªó tr·ª£ t·ª´ ch∆∞a xu·∫•t hi·ªán trong t·∫≠p hu·∫•n luy·ªán (OOV - Out of Vocabulary).
			GloVe (Global Vectors for Word Representation): H·ªçc t·ª´ vector t·ª´ th·ªëng k√™ t·∫ßn su·∫•t ƒë·ªìng xu·∫•t hi·ªán trong t·∫≠p d·ªØ li·ªáu l·ªõn

1.3: 	Deep Learning trong NLP
	L√† m·ªôt nh√°nh c·ªßa Machine Learning (H·ªçc m√°y), trong ƒë√≥ m√¥ h√¨nh s·ª≠ d·ª•ng m·∫°ng n∆°-ron nh√¢n t·∫°o nhi·ªÅu t·∫ßng 
	(Deep Neural Networks - DNNs) ƒë·ªÉ h·ªçc t·ª´ d·ªØ li·ªáu.

1.3.1: 	Deep Neural Networks (DNNs) - M·∫°ng n∆°-ron s√¢u
	M·∫°ng nerual bao g·ªìm nhi·ªÅu l·ªõp nerual nh√¢n t·∫°o ƒë·ªÉ h·ªçc v√† bi·ªÉu di·ªÖn d·ªØ li·ªáu
	1 DNN ƒëi·ªÉn h√¨nh c√≥ c·∫•u tr√∫c nh∆∞ sau:
		Input Layer (L·ªõp ƒë·∫ßu v√†o): Nh·∫≠n d·ªØ li·ªáu ban ƒë·∫ßu: s·ªë, ·∫£nh, vƒÉn b·∫£n, ...
		Hidden Layer (L·ªõp ·∫©n): C√°c t·∫ßng nerual trung gian gi√∫p h·ªçc ƒë·∫∑c tr∆∞ng c∆° b·∫£n
		Output Layer (L·ªõp ƒë·∫ßu ra): Tr·∫£ v·ªÅ d·ª± ƒëo√°n ho·∫∑c ph√¢n lo·∫°i k·∫øt qu·∫£

	1.3.1.1:	FNNs (Feedforward Neural Networks): M·∫°ng truy·ªÅn th·∫≥ng
			L√† d·∫°ng DNN ƒë∆°n gi·∫£n nh·∫•t, d·ªØ li·ªáu di chuy·ªÉn theo 1 h∆∞·ªõng input -> hidden -> output
			·ª®ng d·ª•ng: Ph√¢n lo·∫°i d·ªØ li·ªáu, d·ª± ƒëo√°n gi√° tr·ªã

	1.3.1.2:	Multi-Layer Perceptron (MLP): M·∫°ng Perceptron nhi·ªÅu l·ªõp
			L√† bi·∫øn th·ªÉ c·ªßa FNNs v·ªõi √≠t nh·∫•t 1 hidden layer ; l√† m·∫°ng neural c∆° b·∫£n v·ªõi nhi·ªÅu t·∫ßng (feedforward).	
			·ª®ng d·ª•ng: Ph√¢n lo·∫°i vƒÉn b·∫£n, d·ª± ƒëo√°n chu·ªói th·ªùi gian

1.3.2:	Convolutional Neural Network (CNN): M·∫°ng t√≠ch ch·∫≠p
	M√¥ h√¨nh h·ªçc s√¢u cho x·ª≠ l√Ω ·∫£nh v√† d·ªØ li·ªáu kh√¥ng gian (spatial data)
	Ki·∫øn tr√∫c g·ªìm:
		Convolutional layers: L·ªõp t√≠ch ch·∫≠p; 
			L√† l·ªõp quan tr·ªçng nh·∫•t c·ªßa CNN
			S·ª≠ d·ª•ng b·ªô l·ªçc (filters/kernels) ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh.
			M·ªói b·ªô l·ªçc qu√©t (convolve) qua ·∫£nh ƒë·∫ßu v√†o, gi·ªØ l·∫°i ƒë·∫∑c tr∆∞ng nh∆∞ ƒë∆∞·ªùng vi·ªÅn, g√≥c c·∫°nh,..
			K·∫øt qu·∫£ t·∫°o ra b·∫£n ƒë·ªì ƒë·∫∑c tr∆∞ng (feature maps).
		Pooling layers:	L·ªõp gi·∫£m k√≠ch th∆∞·ªõc
			Gi·∫£m k√≠ch th∆∞·ªõc c·ªßa feature maps, gi√∫p m√¥ h√¨nh b·ªõt ph·ª©c t·∫°p v√† tr√°nh overfitting.
			Ph·ªï bi·∫øn nh·∫•t l√† Max Pooling: L·∫•y gi√° tr·ªã l·ªõn nh·∫•t trong m·ªói v√πng nh·ªè c·ªßa feature map.
		Fully connected layers: L·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß
			Nh·∫≠n ƒë·∫ßu v√†o t·ª´ c√°c feature maps ƒë√£ gi·∫£m k√≠ch th∆∞·ªõc v√† ƒë∆∞a ra d·ª± ƒëo√°n
			Ho·∫°t ƒë·ªông gi·ªëng m·∫°ng n∆°-ron truy·ªÅn th·ªëng (Feedforward Neural Network - FNN)
			S·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t phi tuy·∫øn nh∆∞ ReLU, Softmax ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh
	·ª®ng d·ª•ng CNN trong th·ª±c t·∫ø
		üöó Xe t·ª± l√°i: Nh·∫≠n di·ªán bi·ªÉn b√°o, v·∫°ch ƒë∆∞·ªùng.
		üì∑ Nh·∫≠n di·ªán khu√¥n m·∫∑t: Face ID tr√™n iPhone.
		üî¨ Ch·∫©n ƒëo√°n y t·∫ø: Ph√°t hi·ªán ung th∆∞ t·ª´ ·∫£nh X-ray.
		üõç Ph√°t hi·ªán s·∫£n ph·∫©m trong c·ª≠a h√†ng: Amazon Go.

1.3.3:	Recurrent Neural Network (RNN) - M·∫°ng n∆°-ron h·ªìi quy
	- L√† m·ªôt m·∫°ng n∆°-ron nh√¢n t·∫°o ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu tu·∫ßn t·ª±, ch·∫≥ng h·∫°n nh∆∞ vƒÉn b·∫£n, gi·ªçng n√≥i, chu·ªói th·ªùi gian, ...
	- Kh√°c v·ªõi m·∫°ng n∆°-ron truy·ªÅn th·ªëng Feedforward Neural Networks - FNN, RNN c√≥ b·ªô nh·ªõ (memory) ƒë·ªÉ l∆∞u tr·ªØ th√¥ng tin t·ª´ b∆∞·ªõc tr∆∞·ªõc ƒë√≥,
		gi√∫p n√≥ x·ª≠ l√Ω d·ªØ li·ªáu c√≥ t√≠nh li√™n t·ª•c theo th·ªùi gian
	- M·∫°ng RNN c√≥ c·∫•u tr√∫c l·∫∑p l·∫°i (recurrent structure), trong ƒë√≥ ƒë·∫ßu ra c·ªßa b∆∞·ªõc tr∆∞·ªõc s·∫Ω tr·ªü th√†nh ƒë·∫ßu v√†o c·ªßa b∆∞·ªõc sau
	- V·∫•n ƒë·ªÅ c·ªßa RNN : 
		+ Vanishing Gradient Problem: Khi chu·ªói d·ªØ li·ªáu qu√° d√†i, th√¥ng tin ƒë·∫ßu chu·ªói kh√≥ truy·ªÅn v·ªÅ cu·ªëi,l√†m suy gi·∫£m hi·ªáu su·∫•t l·ªçc
		+ Kh√¥ng x·ª≠ l√Ω t·ªët quan h·ªá d√†i h·∫°n: V√¨ tr·∫°ng th√°i ·∫©n ph·∫£i ch·ª©a th√¥ng tin t·ª´ nhi·ªÅu b∆∞·ªõc tr∆∞·ªõc, vi·ªác l∆∞u tr·ªØ th√¥ng tin l√¢u d√†i g·∫∑p kh√≥ khƒÉn
	- C√°c c·∫£i ti·∫øn c·ªßa RNN: M·∫°ng n∆°-ron h·ªìi quy = Recurrent Neural Network:
		+ Long Short_ Term Memory (LSTM): 
			_ L√† 1 bi·∫øn th·ªÉ c·ªßa RNN, ƒë∆∞·ª£c c·∫£i ti·∫øn gi√∫p m√¥ h√¨nh c√≥ th·ªÉ l∆∞u tr·ªØ th√¥ng tin trong th·ªùi gian d√†i
			d·ª±a tr√™n c√°c c·ªïng ƒëi·ªÅu khi·ªÉn gates
			_ C·∫•u tr√∫c c·ªßa 1 Long Short _ Term Memory (LSTM):
				~ Forget Gate (): Quy·∫øt ƒë·ªãnh gi·ªØ hay b·ªè th√¥ng tin c≈©
				~ Input Gate (): Ki·ªÉm so√°t th√¥ng tin m·ªõi th√™m v√†o b·ªô nh·ªõ
				~ Output Gate (): X√°c ƒë·ªãnh gi√° tr·ªã n√†o ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m ƒë·∫ßu ra
			_ ·ª®ng d·ª•ng c·ªßa LSTM: Long Short_ Term Memory
				~ D·ªãch ng√¥n ng·ªØ (Google Translate)
				~ Nh·∫≠n di·ªán gi·ªçng n√≥i (Siri, Alexa)
				~ Ph√¢n t√≠ch chu·ªói th·ªùi gian (D·ª± ƒëo√°n gi√° c·ªï phi·∫øu)
		+ Gated Recurrent Unit (GRU):
			_ L√† m·ªôt phi√™n b·∫£n ƒë∆°n gi·∫£n h∆°n c·ªßa LSTM (Long Short_ Term Memory) nh∆∞ng v·∫´n duy tr√¨ hi·ªáu su·∫•t cao
			_ ƒêi·ªÉm kh√°c bi·ªát v·ªõi so LSTM (Long Short _ Term Memory) l√† GRU (Gated Recurrent Unit) ch·ªâ c√≥ 2 c·ªïng thay v√¨ 3 c·ªïng  nh∆∞ LSTM (Forger Gate, Input Gate, Output Gate)
				~ Reset Gate: Ki·ªÉm so√°t l∆∞·ª£ng th√¥ng tin c·∫ßn qu√™n
				~ Update Gate : Ki·ªÉm so√°t l∆∞·ª£ng th√¥ng tin c·∫ßn l∆∞u tr·ªØ 
			_ GRU (Gated Recurrent Unit) c√≥ √≠t tham s·ªë h∆°n -> Hu·∫•n luy·ªán nhanh h∆°n v√† hi·ªáu qu·∫£ h∆°n v·ªõi chu·ªói ng·∫Øn
			_ ·ª®ng d·ª•ng c·ªßa GRU (Gated Recurrent Unit):
				~ Chatbots (GPT, tr·ª£ l√Ω ·∫£o)
				~ Nh·∫≠n di·ªán c·∫£m x√∫c trong vƒÉn b·∫£n
				~ D·ª± ƒëo√°n d·ªØ li·ªáu chu·ªói th·ªùi gian 
	
1.3.4: Autoencoders (AE):
	- M·∫°ng nerual nh√¢n t·∫°o (Nerual Network) h·ªçc c√°ch t√°i t·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o b·∫±ng c√°ch n√©n n√≥ xu·ªëng d·∫°ng bi·ªÉu di·ªÖn ti·ªÅm ·∫©n (latent representation), r·ªìi kh√¥i ph·ª•c l·∫°i
	- M·ª•c ƒë√≠ch: 
		~ Gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Reduction): T∆∞∆°ng t·ª± PCA nh∆∞ng m·∫°nh m·∫Ω h∆°n
		~ Ph√°t hi·ªán b·∫•t th∆∞·ªùng (Anomaly Detection): Nh·∫≠n di·ªán l·ªói ho·∫∑c gian l·∫≠n
		~ N√©n d·ªØ li·ªáu (Data Compression): gi·∫£m k√≠ch th∆∞·ªõc m√† v·∫´n gi·ªØ th√¥ng tin quan tr·ªçng
		~ Sinh d·ªØ li·ªáu m·ªõi (Data Generation): Ph·ª•c v·ª• AI s√°ng t·∫°o
	- C·∫•u tr√∫c: 
		~ Encoder: B·ªô m√£ h√≥a: Chuy·ªÉn d·ªØ li·ªáu g·ªëc th√†nh d·∫°ng bi·ªÉu di·ªÖn ti·ªÅm ·∫©n(latent representation)
		~ Decoder: B·ªô gi·∫£i m√£: T√°i t·∫°o d·ªØ li·ªáu t·ª´ latent space
	- C√°c bi·∫øn th·ªÉ c·ªßa Autoencoders (AE):
		+ Denoising Autoencoder (DAE): T·ª± m√£ h√≥a kh·ª≠ nhi·ªÖu
			~ M·ª•c ƒë√≠ch: H·ªçc c√°ch t√°i t·∫°o d·ªØ li·ªáu t·ª´ phi√™n b·∫£n b·ªã nhi·ªÖu
			~ ·ª®ng d·ª•ng: Lo·∫°i b·ªè nhi·ªÖu trong ·∫£nh, vƒÉn b·∫£n
		+ Variational Autoencode (VAE): T·ªØ m√£ h√≥a bi·∫øn th·ªÉ 
			~ M·ª•c ƒë√≠ch: H·ªçc ph√¢n ph·ªëi x√°c su·∫•t d·ªØ li·ªáu, kh√¥ng ch·ªâ t√°i t·∫°o ch√≠nh x√°c m√† c√≤n sinh ra d·ªØ li·ªáu m·ªõi gi·ªëng ƒë·∫ßu v√†o
			~ ·ª®ng d·ª•ng: Sinh ·∫£nh gi·∫£, m√¥ h√¨nh h√≥a d·ªØ li·ªáu 
		+ Sparse Autoencoder: T·ª± m√£ h√≥a th∆∞a
			~ M·ª•c ƒë√≠ch: Th√™m r√†ng bu·ªôc l√†m cho latent representation th∆∞a (ch·ªâ m·ªôt s·ªë √≠t nerual ho·∫°t ƒë·ªông)
			~ ∆Øng d·ª•ng: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng quan tr·ªçng t·ª´ d·ªØ li·ªáu
		+ Convolutional Autoencoder (CAE): T·ª± m√£ h√≥a t√≠ch ch·∫≠p
			~ M·ª•c ƒë√≠ch: X·ª≠ l√Ω ·∫£nh b·∫±ng c√°ch d√πng m·∫°ng CNN thay v√¨ MLPanh
			~ ·ª®ng d·ª•ng: N√©n ·∫£nh, kh·ª≠ nhi·ªÖu, ph·ª•c h·ªìi ·∫£nh b·ªã m·∫•t d·ªØ li·ªáu










